# Тензорные алгоритмы многоруких бандитов

# Описание реализации

# low_ranks_algs

## bandit.py

Представляет собой базовую версию многорукого бандита, который осуществляет взаимодействие со средой, параметры которой заранее заданы.

##### Параметры:

- reward_tesor - задаёт тензор наград, где каждая размерность отвечает за определенный объект выбора

- noise_level - уровень шума, который добавляется для обучения алгоритма в уловиях, приближенных к реальности

- log (по умолчанию False) - при значении True выводит последовательность действий бандитов.


##### Методы:

- PlayArm - принимает на вход номер ручки в виде многомерного индекса (index), осуществляет сэмплирование награды из парметров среды, сохраняет regret

- UpdateArm - принимает номер ручки в аналогичном PlayArm формате, а также delta. В среде увеличивает базовую награду данной ручки на delta.

- GetRegret - возвращает сохраненный вектор ошибок, полезно при сравнении несколькоих алгоритмов

- PlotRegret - сохраняет график regret в файл с заданным именем (принимает как аргумент)


## funcs.py

- L_inf_norm - считает $l_1$-норму тензора

- prod - считает произведение всех элементов любой спископодобной структуры

- marginal_multiplication - считает тензорное произведение, определенное в статье https://arxiv.org/pdf/2007.15788.pdf на странице 9.

## tensor.py

Содержит необходимые классы и функции для операции дополнения тензора. Код был взят из репозитория https://github.com/datamllab/pyten и переписан на Python 3 (оригинальный код на Python 2).

## random.py

##### Параметры:

Реализация алгоритма многоруких бандитов, который на каждом шаге выбирает произвольную ручку, не основывясь на истории действий.

- dimensions -размерность тензора наград среды 
- bandit - объект класса **TensorBandit**, который отвечает за взаимодействие со средой
- total_steps - общее число шагов, доступных алгоритму
- img_name (по умолчанию None, то есть изображение не будет сохранено) - путь к файлу для сохранения графика regret
- update_arm_on_step (по умолчанию None) - номер шага, на котором происходит обновление ручек, правила можно изменить внутри кода
- delete_arm_on_step (по умолчанию None) - номер шага, на котором происходит удаление некоторых ручек, правила можно изменить внутри кода


##### Методы:

- Step - в качестве аргумента получает многомерный индекс ручки, которую играет (передает среде и получает награду)

- UpdateArms - обновляет все ручки, связанные с параметном выбора, который соответствует размерности `dim` и имеет в ней номер `ind`. Увеличивает базовый ревард соответствующих ручек на `delta`.

- DeleteArms - удаляет из рассмотрения ручки, связанные с параметном выбора, который соответствует размерности `dim` и имеет в ней номер `ind`.

- PlayAlgo - запускает весь алгоритм.



## ucb.py

##### Параметры:

Реализация алгоритма многоруких бандитов, который представляет собой векторное обобщение алгоритма UCB. На каждом шаге (после фазы исследования) он выбирает ручку с наибольшим значением верхней границы доверительного интервала, после чего обновляет интервал на основании полученных данных.

- dimensions -размерность тензора наград среды 
- bandit - объект класса **TensorBandit**, который отвечает за взаимодействие со средой
- total_steps - общее число шагов, доступных алгоритму
- explore_steps - количество шагов в фазе исследования (когда алгоритм играет произвольные ручки, чтобы изучить среду).
- img_name (по умолчанию None, то есть изображение не будет сохранено) - путь к файлу для сохранения графика regret
- update_arm_on_step (по умолчанию None) - номер шага, на котором происходит обновление ручек, правила можно изменить внутри кода
- delete_arm_on_step (по умолчанию None) - номер шага, на котором происходит удаление некоторых ручек, правила можно изменить внутри кода


##### Методы:

- Step - в качестве аргумента получает многомерный индекс ручки, которую играет (передает среде и получает награду). Обновляет необходимые алгоритму статистики.

- UpdateArms - обновляет все ручки, связанные с параметном выбора, который соответствует размерности `dim` и имеет в ней номер `ind`. Увеличивает базовый ревард соответствующих ручек на `delta`.

- DeleteArms - удаляет из рассмотрения ручки, связанные с параметном выбора, который соответствует размерности `dim` и имеет в ней номер `ind`.

- FindBestCurrArm - на основании собранных к текущему шагу статистик рассчитывает доверительные интервалы и выбирает ручку с максимальной границей.

- GetArmsRatings - на основании полученной из собранных данных оценки вектора наград присваивает класс от 1 до 5 каждой ручке, где 1 - самым неоптимальным вариантам. Ручкам, которые вызывались менее, чем `unknown_threshold` (по усолчанию 7) раз присваивается класс 0, которые говорит о недостаточном количестве информации.

- PlayAlgo - запускает весь алгоритм.


## elimination.py

##### Параметры:

Реализация Elimination алгоритма многоруких бандитов, описанного в статье https://arxiv.org/pdf/2007.15788.pdf Основная идея алгоритма в том, чтобы постепенно сужать множество рассматриваемых ручек исходя из построение доверительного интервала для возможной награды. В данном случае для оптимизации алгоритм производит оценку не с помощью рассмотрения всех вариантов, а с предположением о ранге тензора наград и используя операцию тензорного дополнения. 

- dimensions -размерность тензора наград среды 
- ranks - предполагаемый ранг тензора наград среды
- bandit - объект класса **TensorBandit**, который отвечает за взаимодействие со средой
- total_steps - общее число шагов, доступных алгоритму
- explore_steps - количество шагов в фазе исследования (когда алгоритм играет произвольные ручки, чтобы изучить среду).
- lambda1 (по умолчанию 0.01) - параметр регуляризации алгоритма (подробнее в статье)
- lambda2 (по умолчанию 20.0) - параметр регуляризации алгоритма (подробнее в статье)
- conf_int_len (по умолчанию 0.3) - длина доверительных интервалов, которые используются в оценке (подробнее в статье)
- img_name (по умолчанию None, то есть изображение не будет сохранено) - путь к файлу для сохранения графика regret
- update_arm_on_step (по умолчанию None) - номер шага, на котором происходит обновление ручек, правила можно изменить внутри кода
- delete_arm_on_step (по умолчанию None) - номер шага, на котором происходит удаление некоторых ручек, правила можно изменить внутри кода


##### Методы:

- Step - в качестве аргумента получает многомерный индекс ручки, которую играет (передает среде и получает награду). Обновляет необходимые алгоритму статистики.

- CreateArmTensorByIndex - вспомогательный метод, который позволяет из многомерного индекса ручки получить тензор, где на соответствующей позиции стоит 1, а на остальных - 0. 

- UpdateArms - обновляет все ручки, связанные с параметном выбора, который соответствует размерности `dim` и имеет в ней номер `ind`. Увеличивает базовый ревард соответствующих ручек на `delta`.

- DeleteArms - удаляет из рассмотрения ручки, связанные с параметном выбора, который соответствует размерности `dim` и имеет в ней номер `ind`.

- FindBestCurrArm - на основании собранных к текущему шагу статистик находит наилучшую ручку согласно алгоритму.

- UpdateEstimation - вызывается каждые 50 (можно изменить в коде) шагов, производит обновление некоторых статистик, которые требуют более длительного времени (обновление разложения таккера и новое тензорное дополнение).

- FindBestBeta - вспомогательный метод, который помогает найти нужный коэффициент для последующей оценки.

- GetArmsRatings - на основании полученной из собранных данных оценки вектора наград присваивает класс от 1 до 5 каждой ручке, где 1 - самым неоптимальным вариантам. Ручкам, которые вызывались менее, чем `unknown_threshold` (по умолчанию 7) раз присваивается класс 0, которые говорит о недостаточном количестве информации.

- PlayAlgo - запускает весь алгоритм.



## epoch_greedy.py

##### Параметры:

Реализация Epoch Greedy алгоритма многоруких бандитов, описанного в статье https://arxiv.org/pdf/2007.15788.pdf . Идея алгоритма состоит в том, чтобы на каждом шаге выбирать ручку, оценка награды которой максимальна. В свою очередь оыенка использует разложение Таккера

- dimensions -размерность тензора наград среды 
- ranks - предполагаемый ранг тензора наград среды
- bandit - объект класса **TensorBandit**, который отвечает за взаимодействие со средой
- total_steps - общее число шагов, доступных алгоритму
- explore_steps - количество шагов в фазе исследования (когда алгоритм играет произвольные ручки, чтобы изучить среду)
- img_name (по умолчанию None, то есть изображение не будет сохранено) - путь к файлу для сохранения графика regret



##### Методы:

- Step - в качестве аргумента получает многомерный индекс ручки, которую играет (передает среде и получает награду). Обновляет необходимые алгоритму статистики.

- CreateArmTensorByIndex - вспомогательный метод, который позволяет из многомерного индекса ручки получить тензор, где на соответствующей позиции стоит 1, а на остальных - 0. 

- FindBestCurrArm - на основании оценки тензора наград выбирает оптимальную ручку, то есть ту, что соответствует макимальному элементу тензора.

- UpdateEstimation - вызывается каждый шаг, производит обновление некоторых оценки тензора наград с помощью разложения Таккера.


- PlayAlgo - запускает весь алгоритм.


## tensor_train.py

##### Параметры:

В данном файле находится реализация нового алгоритма для решения задачи тензорного бандита, который основан на разложении в тензорный поезд. 

- dimensions -размерность тензора наград среды 
- ranks - ранг для разложения в тензорный поезд
- bandit - объект класса **TensorBandit**, который отвечает за взаимодействие со средой
- total_steps - общее число шагов, доступных алгоритму
- explore_steps - количество шагов в фазе исследования (когда алгоритм играет произвольные ручки, чтобы изучить среду)
- k - раз в сколько шагов будет происходить обновление разложения
- img_name (по умолчанию None, то есть изображение не будет сохранено) - путь к файлу для сохранения графика regret


##### Методы:

- Step - в качестве аргумента получает многомерный индекс ручки, которую играет (передает среде и получает награду). Обновляет необходимые алгоритму статистики.

- CreateArmTensorByIndex - вспомогательный метод, который позволяет из многомерного индекса ручки получить тензор, где на соответствующей позиции стоит 1, а на остальных - 0. 

- FindBestCurrArm - на основании оценки тензора наград выбирает оптимальную ручку, то есть ту, что соответствует макимальному элементу тензора. Это делается через разложение в тензорный поезд, без восстановления исходного тензора.

- UpdateEstimation - вызывается раз в k шагов, производит обновление разложения в тензорный поезд, необходимо для уменьшения размерности разложения, которая растёт с каждым шагом.

- PlayAlgo - запускает весь алгоритм.


# context_algs

Содержит котекстуальные версии вышеописанных алгоритмов. Реализации очень схожа с обычной, но использует следующую идею: нужно добавить в тензор наград дополнительные размерности, которые будут кодировать контекст. В общем виде алгоритм будет выглядеть следующим образом. Сначала алгоритм получает контекст, выбирает часть тензора наград, соответствующую ему, и выбирает оптимальную ручку с помощью некоторого тензорного алгоритма.

Таким образом, изменения в классы были внесены без сильного обновления интерфейса.

Дополнительно был реализован алгоритм Ensemble Sampling из https://arxiv.org/pdf/2007.15788.pdf

Он наследует идею Томпсоновского сэмплирования, и работает с обновлением распределения, заданного априорна. В то же время, использует разложение Таккера. Интерфейс соответствует остальным алгоритмам.


# context_algs_open_bandit

Здесь содержаться эксперименты с данными алгоритмами на датасете Open Bandit https://github.com/st-tech/zr-obp/tree/8cbd5fa4558b7ad2ba4781546d6604e4cc3e07c4 . В силу особенностей интерфейса данной библиотеки возникла необходимость видоизменить базовые классы, однако, структура осталась прежней.